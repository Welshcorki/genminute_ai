
import chromadb
import os
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from dotenv import load_dotenv

from langchain_classic.retrievers.self_query.base import SelfQueryRetriever
from langchain_classic.chains.query_constructor.base import AttributeInfo

# í…ìŠ¤íŠ¸ ë¶„í• ì„ ìœ„í•œ import (ì˜ë¯¸ì  ì²­í‚¹ ëŒ€ì•ˆ)
from langchain_text_splitters import RecursiveCharacterTextSplitter
import numpy as np

# Gemini API import
from google import genai


# ì´ íŒŒì¼ì˜ ìƒìœ„ ë””ë ‰í† ë¦¬ì— ìˆëŠ” .env íŒŒì¼ì„ ì°¾ì•„ ë¡œë“œ
dotenv_path = os.path.join(os.path.dirname(__file__), '..', '.env')
load_dotenv(dotenv_path=dotenv_path)

class VectorDBManager:
    COLLECTION_NAMES = {
        'chunks': 'meeting_chunks',
        'subtopic': 'meeting_subtopic',
    }

    def __init__(self, persist_directory="./database/vector_db"):
        if not os.getenv("OPENAI_API_KEY"):
            raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.client = chromadb.PersistentClient(path=persist_directory)
        self.embedding_function = OpenAIEmbeddings()

        # Initialize LLM for SelfQueryRetriever
        self.llm = ChatOpenAI(api_key=os.getenv("OPENAI_API_KEY"), temperature=0)

        self.vectorstores = {
            key: Chroma(
                client=self.client,
                collection_name=name,
                embedding_function=self.embedding_function,
            )
            for key, name in self.COLLECTION_NAMES.items()
        }

        # Define metadata field information for SelfQueryRetriever
        self.metadata_field_infos = {
            "chunks": [
                AttributeInfo(name="meeting_id", description="The unique identifier for the meeting", type="string"),
                AttributeInfo(name="dialogue_id", description="The unique identifier for the dialogue within the meeting", type="string"),
                AttributeInfo(name="chunk_index", description="The index of the chunk within the meeting", type="integer"),
                AttributeInfo(name="title", description="The title of the meeting", type="string"),
                AttributeInfo(name="meeting_date", description="The date of the meeting in ISO format (YYYY-MM-DD)", type="string"),
                AttributeInfo(name="audio_file", description="The name of the audio file for the meeting", type="string"),
                AttributeInfo(name="start_time", description="The start time of the chunk in seconds", type="float"),
                AttributeInfo(name="end_time", description="The end time of the chunk in seconds", type="float"),
                AttributeInfo(name="speaker_count", description="The number of different speakers in the chunk", type="integer"),
            ],
            "subtopic": [
                AttributeInfo(name="meeting_id", description="The unique identifier for the meeting", type="string"),
                AttributeInfo(name="meeting_title", description="The title of the meeting", type="string"),
                AttributeInfo(name="meeting_date", description="The date of the meeting in ISO format (YYYY-MM-DD)", type="string"),
                AttributeInfo(name="audio_file", description="The name of the audio file for the meeting", type="string"),
                AttributeInfo(name="main_topic", description="The main topic of the summarized sub-chunk", type="string"),
                AttributeInfo(name="summaery_index", description="The index of the summary sub-chunk", type="integer"),
            ],
        }

        # Define document content descriptions for SelfQueryRetriever
        self.document_content_descriptions = {
            "chunks": "Semantically grouped chunks of meeting transcript dialogue with speaker labels and timestamps",
            "subtopic": "Summarized sub-topic of a meeting transcript",
        }

        print(f"âœ… VectorDBManager for collections {list(self.COLLECTION_NAMES.values())} initialized.")

    def _clean_text_with_gemini(self, formatted_text: str) -> str:
        """
        Gemini 2.5 Flashë¥¼ ì‚¬ìš©í•´ì„œ [Speaker X, MM:SS] í˜•ì‹ì˜ ì •ë³´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.

        Args:
            formatted_text (str): [Speaker X, MM:SS] í˜•ì‹ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸

        Returns:
            str: ìˆœìˆ˜í•œ ëŒ€í™” í…ìŠ¤íŠ¸ (speakerì™€ ì‹œê°„ ì •ë³´ ì œê±°)
        """
        try:
            api_key = os.environ.get("GOOGLE_API_KEY")
            if not api_key:
                print("âš ï¸ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
                return formatted_text

            client = genai.Client(api_key=api_key)

            prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ [Speaker X, MM:SS] í˜•ì‹ì˜ ëª¨ë“  ì •ë³´ë¥¼ ì œê±°í•˜ê³ , ìˆœìˆ˜í•œ ëŒ€í™” ë‚´ìš©ë§Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ê° ì¤„ì€ í•˜ë‚˜ì˜ ë°œì–¸ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë°œì–¸ ë‚´ìš©ë§Œ ë‚¨ê¸°ê³  í™”ìì™€ ì‹œê°„ ì •ë³´ëŠ” ëª¨ë‘ ì œê±°í•´ì£¼ì„¸ìš”.

ì…ë ¥ ì˜ˆì‹œ:
[Speaker 0, 00:15] ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ íšŒì˜ë¥¼ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.
[Speaker 1, 00:30] ë„¤, ë°˜ê°‘ìŠµë‹ˆë‹¤.

ì¶œë ¥ ì˜ˆì‹œ:
ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ íšŒì˜ë¥¼ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.
ë„¤, ë°˜ê°‘ìŠµë‹ˆë‹¤.

ì²˜ë¦¬í•  í…ìŠ¤íŠ¸:
{formatted_text}

ìˆœìˆ˜í•œ ëŒ€í™” ë‚´ìš©ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ ì—†ì´ ëŒ€í™” ë‚´ìš©ë§Œ ë°˜í™˜í•˜ì„¸ìš”."""

            response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt
            )

            cleaned_text = response.text.strip()
            print(f"âœ… Geminië¡œ í…ìŠ¤íŠ¸ ì •ì œ ì™„ë£Œ (ì›ë³¸ {len(formatted_text)}ì â†’ ì •ì œ {len(cleaned_text)}ì)")
            return cleaned_text

        except Exception as e:
            print(f"âš ï¸ Gemini í…ìŠ¤íŠ¸ ì •ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"ğŸ“ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return formatted_text

    def add_meeting_as_chunk(self, meeting_id, title, meeting_date, audio_file, segments):
        """
        íšŒì˜ ëŒ€í™” ë‚´ìš©ì„ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ì²­í¬ë¡œ ë¬¶ì–´ DBì— ì €ì¥í•©ë‹ˆë‹¤.
        í™”ì ë³€ê²½, ì‹œê°„ ê°„ê²©ì„ ê³ ë ¤í•˜ì—¬ ì²­í‚¹í•˜ë©°, Geminië¥¼ ì‚¬ìš©í•´ì„œ speakerì™€ ì‹œê°„ ì •ë³´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.

        Args:
            meeting_id (str): íšŒì˜ ID
            title (str): íšŒì˜ ì œëª©
            meeting_date (str): íšŒì˜ ì¼ì‹œ
            audio_file (str): ì˜¤ë””ì˜¤ íŒŒì¼ëª…
            segments (list): íšŒì˜ ëŒ€í™” ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
                ê° ì„¸ê·¸ë¨¼íŠ¸ëŠ” {'speaker_label', 'start_time', 'segment', ...} í¬í•¨
        """
        chunk_vdb = self.vectorstores['chunks']

        try:
            # 1. ìŠ¤ë§ˆíŠ¸ ì²­í‚¹: í™”ì ë³€ê²½ê³¼ ì‹œê°„ ê°„ê²©ì„ ê³ ë ¤
            chunks = self._create_smart_chunks(segments, max_chunk_size=1000, time_gap_threshold=60)

            print(f"ğŸ“¦ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ìœ¼ë¡œ {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ")

            # 2. Geminië¡œ ê° ì²­í¬ì˜ í…ìŠ¤íŠ¸ ì •ì œ (speakerì™€ ì‹œê°„ ì •ë³´ ì œê±°)
            print(f"ğŸ¤– Gemini 2.5 Flashë¡œ í…ìŠ¤íŠ¸ ì •ì œ ì¤‘...")
            for chunk in chunks:
                chunk['text'] = self._clean_text_with_gemini(chunk['text'])

            # 3. ê° ì²­í¬ë¥¼ Vector DBì— ì €ì¥
            chunk_texts = []
            chunk_metadatas = []
            chunk_ids = []

            for i, chunk_info in enumerate(chunks):
                chunk_texts.append(chunk_info['text'])
                chunk_metadatas.append({
                    "meeting_id": meeting_id,
                    "dialogue_id": f"{meeting_id}_chunk_{i}",
                    "chunk_index": i,
                    "title": title,
                    "meeting_date": meeting_date,
                    "audio_file": audio_file,
                    "start_time": chunk_info['start_time'],
                    "end_time": chunk_info['end_time'],
                    "speaker_count": chunk_info['speaker_count']
                })
                chunk_ids.append(f"{meeting_id}_chunk_{i}")

            # Vector DBì— ì¶”ê°€
            chunk_vdb.add_texts(
                texts=chunk_texts,
                metadatas=chunk_metadatas,
                ids=chunk_ids
            )

            print(f"âœ… {len(chunks)}ê°œì˜ ìŠ¤ë§ˆíŠ¸ ì²­í¬ë¥¼ meeting_chunks DBì— ì €ì¥ ì™„ë£Œ (meeting_id: {meeting_id})")

        except Exception as e:
            print(f"âš ï¸ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"ğŸ“ ëŒ€ì‹  ê¸°ë³¸ ì²­í‚¹ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

            # ì—ëŸ¬ ë°œìƒ ì‹œ í´ë°±: RecursiveCharacterTextSplitter ì‚¬ìš©
            formatted_segments = []
            for seg in segments:
                speaker = seg.get('speaker_label', 'Unknown')
                start_time = seg.get('start_time', 0)
                text = seg.get('segment', '')
                minutes = int(start_time // 60)
                seconds = int(start_time % 60)
                time_str = f"{minutes:02d}:{seconds:02d}"
                formatted_text = f"[Speaker {speaker}, {time_str}] {text}"
                formatted_segments.append(formatted_text)

            full_text = "\n".join(formatted_segments)

            # RecursiveCharacterTextSplitterë¡œ ì²­í‚¹
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                separators=["\n[Speaker", "\n\n", "\n", " ", ""]
            )

            split_chunks = text_splitter.split_text(full_text)

            # Geminië¡œ í…ìŠ¤íŠ¸ ì •ì œ
            print(f"ğŸ¤– Gemini 2.5 Flashë¡œ í´ë°± í…ìŠ¤íŠ¸ ì •ì œ ì¤‘...")
            cleaned_chunks = [self._clean_text_with_gemini(chunk) for chunk in split_chunks]

            chunk_texts = []
            chunk_metadatas = []
            chunk_ids = []

            for i, chunk_text in enumerate(cleaned_chunks):
                chunk_texts.append(chunk_text)
                chunk_metadatas.append({
                    "meeting_id": meeting_id,
                    "dialogue_id": f"{meeting_id}_chunk_{i}",
                    "chunk_index": i,
                    "title": title,
                    "meeting_date": meeting_date,
                    "audio_file": audio_file
                })
                chunk_ids.append(f"{meeting_id}_chunk_{i}")

            chunk_vdb.add_texts(
                texts=chunk_texts,
                metadatas=chunk_metadatas,
                ids=chunk_ids
            )

            print(f"âœ… {len(split_chunks)}ê°œì˜ ì²­í¬ë¥¼ meeting_chunks DBì— ì €ì¥ ì™„ë£Œ (í´ë°± ëª¨ë“œ)")

    def _create_smart_chunks(self, segments, max_chunk_size=1000, time_gap_threshold=60):
        """
        í™”ì ë³€ê²½, ì‹œê°„ ê°„ê²©ì„ ê³ ë ¤í•œ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹

        Args:
            segments (list): íšŒì˜ ëŒ€í™” ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
            max_chunk_size (int): ìµœëŒ€ ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
            time_gap_threshold (int): ì‹œê°„ ê°„ê²© ì„ê³„ê°’ (ì´ˆ)

        Returns:
            list: ì²­í¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{'text': str, 'start_time': float, 'end_time': float, 'speaker_count': int}]
        """
        chunks = []
        current_chunk = []
        current_chunk_text = ""
        current_speaker = None
        last_time = 0
        speakers_in_chunk = set()

        for seg in segments:
            speaker = seg.get('speaker_label', 'Unknown')
            start_time = seg.get('start_time', 0)
            text = seg.get('segment', '')

            # ì‹œê°„ì„ MM:SS í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            minutes = int(start_time // 60)
            seconds = int(start_time % 60)
            time_str = f"{minutes:02d}:{seconds:02d}"

            # í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸
            formatted_text = f"[Speaker {speaker}, {time_str}] {text}"

            # ì²­í¬ ë¶„ë¦¬ ì¡°ê±´:
            # 1. í˜„ì¬ ì²­í¬ í¬ê¸°ê°€ max_chunk_sizeë¥¼ ì´ˆê³¼
            # 2. ì‹œê°„ ê°„ê²©ì´ time_gap_threshold ì´ˆê³¼ (ê¸´ ì¹¨ë¬µì´ë‚˜ ì£¼ì œ ì „í™˜ ê°€ëŠ¥ì„±)
            # 3. í™”ìê°€ ë³€ê²½ë˜ê³  í˜„ì¬ ì²­í¬ê°€ ì¶©ë¶„íˆ í¼ (500ì ì´ìƒ)

            time_gap = start_time - last_time
            should_split = False

            if len(current_chunk_text) + len(formatted_text) > max_chunk_size:
                should_split = True
            elif time_gap > time_gap_threshold and len(current_chunk_text) > 200:
                should_split = True
            elif speaker != current_speaker and len(current_chunk_text) > 500:
                # í™”ì ë³€ê²½ ì‹œ ì ë‹¹í•œ í¬ê¸°ë©´ ë¶„ë¦¬
                should_split = True

            if should_split and current_chunk:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                chunks.append({
                    'text': current_chunk_text.strip(),
                    'start_time': current_chunk[0].get('start_time', 0),
                    'end_time': current_chunk[-1].get('start_time', 0),
                    'speaker_count': len(speakers_in_chunk)
                })

                # ìƒˆ ì²­í¬ ì‹œì‘
                current_chunk = []
                current_chunk_text = ""
                speakers_in_chunk = set()

            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€
            current_chunk.append(seg)
            current_chunk_text += formatted_text + "\n"
            speakers_in_chunk.add(speaker)
            current_speaker = speaker
            last_time = start_time

        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunks.append({
                'text': current_chunk_text.strip(),
                'start_time': current_chunk[0].get('start_time', 0),
                'end_time': current_chunk[-1].get('start_time', 0),
                'speaker_count': len(speakers_in_chunk)
            })

        return chunks


    def add_meeting_as_subtopic(self, meeting_id, title, meeting_date, audio_file, summary_content):
        """ìŠ¤í¬ë¦½íŠ¸ ì „ì²´ë¥¼ ì†Œì£¼ì œë³„ ì²­í¬ë¡œ DBì— ì €ì¥í•©ë‹ˆë‹¤."""

        
        # 1. ìƒì„±ëœ ìš”ì•½ì„ ì£¼ì œë³„ë¡œ íŒŒì‹±
        # "### "ë¡œ ë¶„ë¦¬í•˜ë˜, ì²« ë²ˆì§¸ ìš”ì†Œê°€ ê³µë°±ì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ filter(None, ...) ì‚¬ìš©
        summary_chunks = summary_content.split('\n### ')
        summary_chunks = [chunk.strip() for chunk in summary_chunks if chunk.strip()]
        
        # ì²« ë²ˆì§¸ ì²­í¬ì— "### "ê°€ ëˆ„ë½ë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì²« ë²ˆì§¸ ì²­í¬ë§Œ ë”°ë¡œ ì²˜ë¦¬
        # if summary_chunks and not summary_chunks[0].startswith('###'):
        #      # ì²«ë²ˆì§¸ ì²­í¬ê°€ ###ë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ###ë¥¼ ë¶™ì—¬ì¤€ë‹¤.
        #      if summary_chunks[0].count('\n') > 0:
        #          summary_chunks[0] = '### ' + summary_chunks[0]

        print("===============summary_chunks=================")
        print(summary_chunks)
        
        # 2. ê° ìš”ì•½ chunkë¥¼ Summary_Analysis_DBì— ì €ì¥
        subtopic_vdb = self.vectorstores['subtopic']
        chunk_texts = []
        chunk_metadatas = []
        chunk_ids = []

        for i, chunk in enumerate(summary_chunks):
            # '### 'ê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬, ì²« ì¤„ì„ main_topicìœ¼ë¡œ ì¶”ì¶œ
            lines = chunk.split('\n')
            main_topic = lines[0].replace('### ', '').strip()
            
            # ì‹¤ì œ ì €ì¥ë  ë‚´ìš©ì€ '### 'ë¥¼ í¬í•¨í•œ ì „ì²´ ì²­í¬
            full_chunk_content = '### ' + chunk if not chunk.startswith('###') else chunk

            chunk_texts.append(full_chunk_content)
            chunk_metadatas.append({
                "meeting_id": meeting_id,
                "meeting_title": title,
                "meeting_date": meeting_date,
                "audio_file": audio_file,
                "main_topic": main_topic,
                "summary_index": i
            })
            chunk_ids.append(f"{meeting_id}_summary_{i}")

        if chunk_texts:
            subtopic_vdb.add_texts(texts=chunk_texts, metadatas=chunk_metadatas, ids=chunk_ids)
            print(f"ğŸ“„ ìš”ì•½ ê²°ê³¼ {len(chunk_texts)}ê°œë¥¼ Summary_Analysis_DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            return summary_chunks
        else:
            print("âš ï¸ ìš”ì•½ ê²°ê³¼ì—ì„œ ìœ íš¨í•œ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")



    
    
    def search(self,
             db_type: str,
             query: str,
             k: int = 5,
             retriever_type: str = "similarity",
             filter_criteria: dict = None,
             score_threshold: float = None,  # <-- [ìˆ˜ì •ë¨] ì ìˆ˜ ì„ê³„ê°’ ì¶”ê°€
             mmr_fetch_k: int = 20,         # <-- [ìˆ˜ì •ë¨] MMR fetch_k ì¶”ê°€
             mmr_lambda_mult: float = 0.5   # <-- [ìˆ˜ì •ë¨] MMR lambda_mult ì¶”ê°€
             ) -> list:
        """
        ì§€ì •ëœ DBì—ì„œ ì¿¼ë¦¬ì™€ í•„í„° ì¡°ê±´ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        score_thresholdê°€ ì§€ì •ë˜ë©´ retriever_typeì€ 'similarity_score_threshold'ë¡œ ìë™ ë³€ê²½ë©ë‹ˆë‹¤.

        Args:
            db_type (str): ê²€ìƒ‰í•  DB íƒ€ì… ('chunks', 'subtopic').
            query (str): ê²€ìƒ‰í•  í…ìŠ¤íŠ¸ ì¿¼ë¦¬.
            k (int, optional): ë°˜í™˜í•  ê²°ê³¼ì˜ ìˆ˜. Defaults to 5.
            retriever_type (str, optional): ì‚¬ìš©í•  ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì… ('similarity', 'mmr', 'self_query', 'similarity_score_threshold'). Defaults to "similarity".
            filter_criteria (dict, optional): ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì¡°ê±´ (ì˜ˆ: {'meeting_id': '...', 'audio_file': '...'}). Defaults to None.
            score_threshold (float, optional): ìœ ì‚¬ë„ ì ìˆ˜ ì„ê³„ê°’ (0.0~1.0). Defaults to None.
            mmr_fetch_k (int, optional): MMRì—ì„œ ì´ˆê¸° fetchí•  ë¬¸ì„œ ìˆ˜. Defaults to 20.
            mmr_lambda_mult (float, optional): MMRì˜ ë‹¤ì–‘ì„± íŒŒë¼ë¯¸í„° (0.0~1.0). Defaults to 0.5.

        Returns:
            list: LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸.
        """
        # 1. Validate inputs
        if db_type not in self.vectorstores:
            raise ValueError(f"Unknown db_type: {db_type}. Available types are {list(self.vectorstores.keys())}")

        # [ìˆ˜ì •ë¨] "similarity_score_threshold"ë¥¼ ìœ íš¨í•œ íƒ€ì…ìœ¼ë¡œ í—ˆìš©
        allowed_types = ["similarity", "mmr", "self_query", "similarity_score_threshold"]
        if retriever_type not in allowed_types:
            raise ValueError(f"Unsupported retriever_type: {retriever_type}. Choose from {allowed_types}.")

        # [ìˆ˜ì •ë¨] score_thresholdê°€ ì œê³µë˜ë©´, retriever_typeì„ ê°•ì œë¡œ ë³€ê²½
        current_retriever_type = retriever_type
        if score_threshold is not None and retriever_type == "similarity":
            current_retriever_type = "similarity_score_threshold"
            print(f"â„¹ï¸ score_threshold provided. Changing retriever_type to 'similarity_score_threshold'.")

        vdb = self.vectorstores[db_type]
        results = []

        # 2. Handle 'similarity', 'mmr', 'similarity_score_threshold' retrievers
        if current_retriever_type in ["similarity", "mmr", "similarity_score_threshold"]:
            search_kwargs = {'k': k}
            if filter_criteria:
                search_kwargs['filter'] = filter_criteria

            # [ìˆ˜ì •ë¨] íƒ€ì…ì— ë”°ë¼ search_kwargs ë™ì  êµ¬ì„±
            if current_retriever_type == "similarity_score_threshold":
                if score_threshold is None:
                    raise ValueError("score_threshold must be provided when retriever_type is 'similarity_score_threshold'")
                search_kwargs['score_threshold'] = score_threshold

            elif current_retriever_type == "mmr":
                search_kwargs['fetch_k'] = mmr_fetch_k
                search_kwargs['lambda_mult'] = mmr_lambda_mult

            retriever = vdb.as_retriever(
                search_type=current_retriever_type,
                search_kwargs=search_kwargs
            )
            results = retriever.invoke(query)

        # 3. Handle 'self_query' retriever
        elif current_retriever_type == "self_query":
            # (ì°¸ê³ : SelfQueryRetrieverëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë‚´ë¶€ì—ì„œ similarity_searchë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.)
            # (ì—¬ê¸°ì„œ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§ì„ í•˜ë ¤ë©´, SelfQueryRetrieverë¥¼ ì»¤ìŠ¤í…€í•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.)
            metadata_info = self.metadata_field_infos[db_type]
            doc_description = self.document_content_descriptions[db_type]

            retriever = SelfQueryRetriever.from_llm(
                self.llm,
                vdb,
                doc_description,
                metadata_info,
                verbose=True,
                base_filter=filter_criteria,
                # [ê°œì„  ì•„ì´ë””ì–´] SelfQueryRetrieverê°€ ë°˜í™˜í•  kì˜ ê°œìˆ˜ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                # ë‹¤ë§Œ, invoke ì‹œì ì´ ì•„ë‹Œ ìƒì„± ì‹œì ì— search_kwargsë¥¼ ë„˜ê²¨ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (LangChain ë²„ì „ì— ë”°ë¼ ë‹¤ë¦„)
                # enable_limit=Trueë¥¼ ì‚¬ìš©í•˜ê³  ì¿¼ë¦¬ì— "top 3 results" ë“±ì„ í¬í•¨ì‹œì¼œì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
            )
            results = retriever.invoke(query)

            # [ìˆ˜ì •ë¨] SelfQuery ì´í›„ì—ë„ kê°œë§Œ ë°˜í™˜í•˜ë„ë¡ ê°•ì œ (í•„ìš”ì‹œ)
            # SelfQueryRetrieverëŠ” kë¥¼ LLMì´ ì¶”ë¡ í•˜ê²Œ í•˜ë¯€ë¡œ, kê°€ ë¬´ì‹œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # ë§Œì•½ ê²°ê³¼ê°€ ë„ˆë¬´ ë§ë‹¤ë©´, kë§Œí¼ ì˜ë¼ëƒ…ë‹ˆë‹¤.
            if len(results) > k:
                 print(f"â„¹ï¸ SelfQuery found {len(results)} results. Truncating to k={k}.")
                 results = results[:k]

        print(f"âœ… Found {len(results)} documents from '{self.COLLECTION_NAMES[db_type]}' for query: '{query}'")
        return results

    
    def get_summary_by_meeting_id(self, meeting_id: str) -> str:
        """
        meeting_idë¡œ ë¬¸ë‹¨ ìš”ì•½ì„ summary_index ìˆœì„œëŒ€ë¡œ ê°€ì ¸ì™€ì„œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©í•©ë‹ˆë‹¤.

        Args:
            meeting_id (str): íšŒì˜ ID

        Returns:
            str: summary_index ìˆœì„œëŒ€ë¡œ ê²°í•©ëœ ì „ì²´ ë¬¸ë‹¨ ìš”ì•½ í…ìŠ¤íŠ¸
                 (ìš”ì•½ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜)
        """
        try:
            # meeting_subtopic ì»¬ë ‰ì…˜ì—ì„œ í•´ë‹¹ meeting_idì˜ ëª¨ë“  ì²­í¬ ì¡°íšŒ
            collection = self.client.get_collection(name=self.COLLECTION_NAMES['subtopic'])

            # meeting_idë¡œ í•„í„°ë§í•˜ì—¬ ëª¨ë“  í•­ëª© ê°€ì ¸ì˜¤ê¸°
            results = collection.get(
                where={"meeting_id": meeting_id},
                include=["documents", "metadatas"]
            )

            if not results or not results.get('documents'):
                print(f"âš ï¸ meeting_id '{meeting_id}'ì— ëŒ€í•œ ë¬¸ë‹¨ ìš”ì•½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return ""

            # documentsì™€ metadatasë¥¼ summary_index ìˆœì„œë¡œ ì •ë ¬
            documents = results['documents']
            metadatas = results['metadatas']

            # (summary_index, document) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„± í›„ ì •ë ¬
            indexed_docs = []
            for doc, meta in zip(documents, metadatas):
                summary_index = meta.get('summary_index', 0)
                indexed_docs.append((summary_index, doc))

            # summary_index ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            indexed_docs.sort(key=lambda x: x[0])

            # ë¬¸ì„œë“¤ì„ ìˆœì„œëŒ€ë¡œ ê²°í•© (ê° ë¬¸ì„œ ì‚¬ì´ì— ì¤„ë°”ê¿ˆ 2ê°œ ì¶”ê°€)
            full_summary = "\n\n".join([doc for _, doc in indexed_docs])

            print(f"âœ… meeting_id '{meeting_id}'ì— ëŒ€í•œ {len(indexed_docs)}ê°œì˜ ë¬¸ë‹¨ ìš”ì•½ì„ ìˆœì„œëŒ€ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
            return full_summary

        except Exception as e:
            print(f"âŒ ë¬¸ë‹¨ ìš”ì•½ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return ""

    def delete_from_collection(self, db_type, meeting_id=None, audio_file=None, title=None):
        """
        ì§€ì •ëœ ë²¡í„° DB ì»¬ë ‰ì…˜ì—ì„œ í•­ëª©ì„ ì‚­ì œí•©ë‹ˆë‹¤.
        meeting_id, audio_file, title ì¤‘ í•˜ë‚˜ ì´ìƒì´ ì œê³µë˜ë©´ í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” í•­ëª©ì„ ì‚­ì œí•©ë‹ˆë‹¤.
        ì•„ë¬´ê²ƒë„ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ í•´ë‹¹ db_typeì˜ ì „ì²´ ì»¬ë ‰ì…˜ì„ ì‚­ì œí•©ë‹ˆë‹¤.
        """
        if db_type not in self.vectorstores:
            raise ValueError(f"Unknown db_type: {db_type}. Must be one of {list(self.COLLECTION_NAMES.keys())}")

        collection = self.client.get_or_create_collection(name=self.COLLECTION_NAMES[db_type])

        filters = {}
        if meeting_id:
            filters["meeting_id"] = meeting_id
        if audio_file:
            filters["audio_file"] = audio_file
        if title:
            filters["title"] = title

        if filters:
            # íŠ¹ì • í•„í„°ê°€ ìˆëŠ” ê²½ìš°
            print(f"ğŸ—‘ï¸ Deleting from '{db_type}' collection with filters: {filters}")
            collection.delete(where=filters)
            print(f"âœ… Deletion from '{db_type}' collection complete.")
        else:
            # í•„í„°ê°€ ì—†ëŠ” ê²½ìš°, ì „ì²´ ì»¬ë ‰ì…˜ ì‚­ì œ
            print(f"âš ï¸ No specific filters provided. Deleting ALL items from '{db_type}' collection.")
            collection.delete(where={}) # deletes all items
            print(f"âœ… All items deleted from '{db_type}' collection.")



# --- ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ---
# DB íŒŒì¼ì€ minute_ai/database/vector_db ê²½ë¡œì— ì €ì¥ë©ë‹ˆë‹¤.
# vector_db_path = os.path.join(os.path.dirname(__file__), '..', 'database', 'vector_db')
vdb_manager = VectorDBManager()
